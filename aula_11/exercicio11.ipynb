{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar requisitos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow>=2.15.0 tqdm>=4.66 torch==2.4.0 transformers tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixar pesos do GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 13:38:14.070647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 13:38:14.086240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739551094.105762  737646 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739551094.112202  737646 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 13:38:14.132665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Download script to download the GPT2 weights\n",
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Download GPT2 weights\n",
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar se baixou corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Chaves dos dicionários de parâmetros: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Dimensão do tensor de pesos: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "# Test the GPT2 weights\n",
    "print(\"Configuração:\", settings)\n",
    "print(\"Chaves dos dicionários de parâmetros:\", params.keys())\n",
    "print(params[\"wte\"])\n",
    "print(\"Dimensão do tensor de pesos:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar um modelo qualquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado pierreguillou/gpt2-small-portuguese com a configuração GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try to load the GPT2 model\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"pierreguillou/gpt2-small-portuguese\"\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "config.update({\n",
    "    \"n_ctx\": 1024,  # Length\n",
    "    \"n_layer\": 12,  # Number of layers\n",
    "    \"n_head\": 12,   # Number of heads\n",
    "    \"n_embd\": 768,  # Embedding dimension\n",
    "})\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "model.eval()\n",
    "\n",
    "# Verify the model\n",
    "print(f\"Modelo carregado {model_name} com a configuração {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustar configurações do modelo qualquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo com configuração ajustada: GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"dropout\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"qkv_bias\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary of GPT model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"n_embd\": 768, \"n_layer\": 12, \"n_head\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"n_embd\": 1024, \"n_layer\": 24, \"n_head\": 16},\n",
    "    \"gpt2-large (774M)\": {\"n_embd\": 1280, \"n_layer\": 36, \"n_head\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"n_embd\": 1600, \"n_layer\": 48, \"n_head\": 25},\n",
    "}\n",
    "\n",
    "# Not trained model config\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"n_ctx\": 256,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12, \n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# Set the model name to test\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "\n",
    "# Merge configs\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"n_ctx\": 1024})     # Token length\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})  # Bias for consistency\n",
    "\n",
    "# Model using the modified config\n",
    "new_config_obj = GPT2Config(**NEW_CONFIG)\n",
    "gpt = GPT2LMHeadModel(new_config_obj)\n",
    "gpt.eval()\n",
    "\n",
    "print(\"Modelo com configuração ajustada:\", new_config_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para transferir os pesos e gerar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# Return the right tensor as treinable\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right, dtype=torch.float32))\n",
    "\n",
    "# Load weights to our GPT model\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    # 🔹 Correct positional and token embeddings\n",
    "    gpt.transformer.wpe.weight = assign(gpt.transformer.wpe.weight, params['wpe'])\n",
    "    gpt.transformer.wte.weight = assign(gpt.transformer.wte.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # Split the concatenated query, key, value matrices\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        gpt.transformer.h[b].attn.c_attn.weight = assign(\n",
    "            gpt.transformer.h[b].attn.c_attn.weight, np.concatenate([q_w, k_w, v_w], axis=-1)\n",
    "        )\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        gpt.transformer.h[b].attn.c_attn.bias = assign(\n",
    "            gpt.transformer.h[b].attn.c_attn.bias, np.concatenate([q_b, k_b, v_b], axis=-1)\n",
    "        )\n",
    "        gpt.transformer.h[b].attn.c_proj.weight = assign(\n",
    "            gpt.transformer.h[b].attn.c_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].attn.c_proj.bias = assign(\n",
    "            gpt.transformer.h[b].attn.c_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].mlp.c_fc.weight = assign(\n",
    "            gpt.transformer.h[b].mlp.c_fc.weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].mlp.c_fc.bias = assign(\n",
    "            gpt.transformer.h[b].mlp.c_fc.bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].mlp.c_proj.weight = assign(\n",
    "            gpt.transformer.h[b].mlp.c_proj.weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].mlp.c_proj.bias = assign(\n",
    "            gpt.transformer.h[b].mlp.c_proj.bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].ln_1.weight = assign(\n",
    "            gpt.transformer.h[b].ln_1.weight, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].ln_1.bias = assign(\n",
    "            gpt.transformer.h[b].ln_1.bias, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].ln_2.weight = assign(\n",
    "            gpt.transformer.h[b].ln_2.weight, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.transformer.h[b].ln_2.bias = assign(\n",
    "            gpt.transformer.h[b].ln_2.bias, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "    # Final layer norm (ln_f)\n",
    "    gpt.transformer.ln_f.weight = assign(gpt.transformer.ln_f.weight, params[\"g\"])\n",
    "    gpt.transformer.ln_f.bias = assign(gpt.transformer.ln_f.bias, params[\"b\"])\n",
    "\n",
    "    # Output head (tied with token embeddings)\n",
    "    gpt.lm_head.weight = assign(gpt.lm_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, context_size, top_k=50, temperature=1.0):\n",
    "\n",
    "    # Move input tensor to device\n",
    "    device = next(model.parameters()).device  \n",
    "    idx = idx.clone().detach().to(device) \n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Model's context size\n",
    "        input_tensor = idx[:, -context_size:]\n",
    "\n",
    "        # Get logits\n",
    "        logits = model(input_tensor).logits\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Temperature\n",
    "        logits /= temperature\n",
    "\n",
    "        # Keep top-k probabilities\n",
    "        if top_k > 0:\n",
    "            values, indices = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, -1]] = -float(\"Inf\")\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Sample next token\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        # Append\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciar o modelo pegando camada do GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to load the GPT 2 weights into our model\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída:\n",
      " My NLP professor is the dean's assistant, helping students with the student writing requirement, student engagement with their institution and learning as you move up.\n"
     ]
    }
   ],
   "source": [
    "# Generate some text just to test it\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"My NLP professor is\", tokenizer).to('cuda'),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"n_ctx\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Saída:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
